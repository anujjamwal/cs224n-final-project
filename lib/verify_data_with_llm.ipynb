{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5a9d94b",
   "metadata": {},
   "source": [
    "# Verification Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af728664",
   "metadata": {},
   "source": [
    "This notebook verifies the hierarchical CoT dataset by:\n",
    "1. Loading a small language model and running vanilla inference as a baseline\n",
    "2. Fine-tuning the model on hierarchical CoT data with `[THOUGHT]`, `[SOLUTION]`, `[RETURN]` special tokens\n",
    "3. Running inference with a custom generate loop that prunes intermediate reasoning when `[RETURN]` is produced\n",
    "4. Comparing token efficiency and answer quality between vanilla and hierarchical approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d983ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d028a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import model\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe73bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook_login()\n",
    "os.environ[\"HF_TOKEN\"]=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2de963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU 0 Memory Status:\n",
      "  Total Memory: 81152.75 MiB\n",
      "  Allocated Memory: 0.00 MiB\n",
      "  Reserved (Cached) Memory: 0.00 MiB\n",
      "  Free in Cache: 0.00 MiB\n",
      "\n",
      "Detailed Memory Summary:\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    _dev_idx = torch.cuda.current_device()\n",
    "    total_memory = torch.cuda.get_device_properties(_dev_idx).total_memory\n",
    "    allocated_memory = torch.cuda.memory_allocated(_dev_idx)\n",
    "    reserved_memory = torch.cuda.memory_reserved(_dev_idx)\n",
    "\n",
    "    print(f\"GPU {_dev_idx} Memory Status:\")\n",
    "    print(f\"  Total Memory: {total_memory / (1024**2):.2f} MiB\")\n",
    "    print(f\"  Allocated Memory: {allocated_memory / (1024**2):.2f} MiB\")\n",
    "    print(f\"  Reserved (Cached) Memory: {reserved_memory / (1024**2):.2f} MiB\")\n",
    "    print(f\"  Free in Cache: {(reserved_memory - allocated_memory) / (1024**2):.2f} MiB\")\n",
    "\n",
    "    print(\"\\nDetailed Memory Summary:\")\n",
    "    print(torch.cuda.memory_summary(device=_dev_idx, abbreviated=True))\n",
    "else:\n",
    "    print(\"CUDA not available.\")\n",
    "\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"nvidia/OpenMath-Nemotron-1.5B\"\n",
    "DATASET_NAME = \"anujjamwal/openmathreasoning-hierarchical-cot\"\n",
    "MAX_NEW_TOKENS = 8192\n",
    "MAX_SEQ_LENGTH = 32768\n",
    "PROMPT_TEMPLATE = \"Solve the following math problem. Make sure to put the answer (and only answer) inside \\\\boxed{{}}.\\\\n\\\\n{problem}\"\n",
    "\n",
    "# Checkpoint configuration\n",
    "CHECKPOINT_DIR = \"./checkpoints/hierarchical-cot\"\n",
    "HF_CHECKPOINT_REPO = \"anujjamwal/OpenMath-Nemotron-1.5B-hierarchical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97fe21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clear_cache(*extra_vars):\n",
    "    \"\"\"Free GPU memory: delete named globals, run Python GC, flush CUDA allocator cache.\n",
    "\n",
    "    gc.collect() MUST run before empty_cache(): Python needs to decrement reference\n",
    "    counts first so tensors become truly unreferenced before the CUDA allocator can\n",
    "    reclaim those pages.  Calling empty_cache() alone (as was done previously) has\n",
    "    no effect on live Python objects still holding tensor references.\n",
    "    \"\"\"\n",
    "    _defaults = ('hier_model', 'base_model', 'optimizer', 'vanilla_model')\n",
    "    for var in _defaults + tuple(extra_vars):\n",
    "        if var in globals():\n",
    "            del globals()[var]\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6294b9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a450996bf9124dec803b2dd6f596492a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3821090913.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m ).to(device)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model: {MODEL_NAME}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3585\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3586\u001b[0m                 )\n\u001b[0;32m-> 3587\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     def register_full_backward_pre_hook(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1355\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m                     )\n\u001b[0;32m-> 1357\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1358\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and base model (before adding special tokens so vanilla inference is unaffected)\n",
    "vanilla_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if vanilla_tokenizer.pad_token is None:\n",
    "    vanilla_tokenizer.pad_token = vanilla_tokenizer.eos_token\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "# low_cpu_mem_usage=True loads weights onto a meta device first, then\n",
    "# materialises each tensor individually in bfloat16 — peak CPU RAM is\n",
    "# ~1× model size instead of ~2×.  Then .to(device) moves everything to GPU\n",
    "# in one pass.  This avoids the accelerate dispatch hooks used by\n",
    "# device_map=\"auto\", which were placing the model on CPU instead of GPU.\n",
    "vanilla_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in vanilla_model.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"Vocab size: {len(vanilla_tokenizer)}\")\n",
    "print(f\"Model device: {next(vanilla_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4518b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: anujjamwal/openmathreasoning-hierarchical-cot\n",
      "Total examples: 10\n",
      "Columns: ['expected_answer', 'problem_type', 'problem_source', 'generation_model', 'pass_rate_72b_tir', 'problem', 'generated_solution', 'inference_mode', 'used_in_kaggle', 'hierarchical_cot']\n",
      "\n",
      "Train: 9 examples\n",
      "Test:  1 examples\n",
      "\n",
      "--- Example Problem (truncated) ---\n",
      "Calculate the integral\n",
      "\n",
      "\\[\n",
      "\\int^{\\frac{3\\pi}{2}}_{\\frac{\\pi}{2}} \\left|\\left(\\frac{2}{x^3}+\\frac{1}{x}\\right)\\sin x\\right|dx\n",
      "\\]\n",
      "\n",
      "--- Expected Answer ---\n",
      "\\(\\frac{2}{\\pi} + \\frac{32}{9\\pi^2}\\)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Total examples: {len(dataset)}\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "\n",
    "# Split into train (90%) and test (10%)\n",
    "split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_data = split[\"train\"]\n",
    "test_data = split[\"test\"]\n",
    "\n",
    "print(f\"\\nTrain: {len(train_data)} examples\")\n",
    "print(f\"Test:  {len(test_data)} examples\")\n",
    "\n",
    "# Preview a test example\n",
    "example = test_data[0]\n",
    "print(f\"\\n--- Example Problem (truncated) ---\")\n",
    "print(example[\"problem\"][:300])\n",
    "print(f\"\\n--- Expected Answer ---\")\n",
    "print(example[\"expected_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6003abjpjcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_boxed_answer(text):\n",
    "    \"\"\"Extract the answer from the last \\\\boxed{...} in text, handling nested braces.\"\"\"\n",
    "    idx = text.rfind(\"\\\\boxed{\")\n",
    "    if idx == -1:\n",
    "        return None\n",
    "    idx += len(\"\\\\boxed{\")\n",
    "    depth = 1\n",
    "    end = idx\n",
    "    while end < len(text) and depth > 0:\n",
    "        if text[end] == \"{\":\n",
    "            depth += 1\n",
    "        elif text[end] == \"}\":\n",
    "            depth -= 1\n",
    "        end += 1\n",
    "    return text[idx : end - 1] if depth == 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee68e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d7102a27ef4deaa70c696bab06dd71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vanilla inference:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3895432748.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# inputs = tokenizer(prompt, return_tensors=\"pt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvanilla_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_torch_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             self.data = {\n\u001b[0;32m--> 774\u001b[0;31m                 \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             }\n",
      "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Execute vanilla model with each example and capture token counts\n",
    "vanilla_results = []\n",
    "vanilla_model.eval()\n",
    "\n",
    "for i, example in enumerate(tqdm(train_data, desc=\"Vanilla inference\")):\n",
    "    problem = example[\"problem\"]\n",
    "    prompt = PROMPT_TEMPLATE.format(problem=problem)\n",
    "\n",
    "    # inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = vanilla_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = vanilla_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False,\n",
    "        )\n",
    "\n",
    "\n",
    "    generated_tokens = outputs.shape[1] - input_length\n",
    "    output_text = vanilla_tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\n",
    "    predicted_answer = extract_boxed_answer(output_text)\n",
    "\n",
    "    vanilla_results.append(\n",
    "        {\n",
    "            \"problem\": problem,\n",
    "            \"output\": output_text,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"expected_answer\": example[\"expected_answer\"],\n",
    "            \"tokens_generated\": generated_tokens,\n",
    "            \"total_context_length\": outputs.shape[1],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    del output_text\n",
    "    del outputs\n",
    "    del inputs\n",
    "\n",
    "    print(f\"\\n[Example {i}]\")\n",
    "    print(f\"  Problem:   {problem[:100]}...\")\n",
    "    print(f\"  Predicted: {predicted_answer}\")\n",
    "    print(f\"  Expected:  {example['expected_answer'][:80]}\")\n",
    "    print(f\"  Tokens generated: {generated_tokens}\")\n",
    "\n",
    "print(f\"\\n--- Vanilla Summary ---\")\n",
    "print(\n",
    "    f\"Avg tokens generated:  {np.mean([r['tokens_generated'] for r in vanilla_results]):.1f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Avg context length:    {np.mean([r['total_context_length'] for r in vanilla_results]):.1f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd56c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlexAttention available — using memory-efficient block-sparse masking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8388a1b7402b47d2a341ec3ea36dc361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2347833507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m ).to(device)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model device: {next(base_model.parameters()).device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3585\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3586\u001b[0m                 )\n\u001b[0;32m-> 3587\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     def register_full_backward_pre_hook(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1355\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m                     )\n\u001b[0;32m-> 1357\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1358\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Load Model For Training\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "try:\n",
    "    from torch.nn.attention.flex_attention import create_block_mask\n",
    "    _attn_impl = \"flex_attention\"\n",
    "    print(\"FlexAttention available — using memory-efficient block-sparse masking\")\n",
    "except ImportError:\n",
    "    _attn_impl = \"sdpa\"\n",
    "    print(\"WARNING: FlexAttention not available (requires PyTorch >= 2.3).\")\n",
    "    print(\"         Falling back to SDPA. Training will OOM at seq_len > ~4096.\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    attn_implementation=_attn_impl,\n",
    "    dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\n",
    ")\n",
    "\n",
    "print(f\"Model device: {next(base_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48546a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3 special tokens: ['[THOUGHT]', '[SOLUTION]', '[RETURN]']\n",
      "New vocab size: 151668\n",
      "No checkpoint found — starting fresh training\n",
      "[THOUGHT] token id: 151665\n",
      "[SOLUTION] token id: 151666\n",
      "[RETURN] token id: 151667\n"
     ]
    }
   ],
   "source": [
    "# --- Add special tokens and prepare for hierarchical training ---\n",
    "num_added = tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": SPECIAL_TOKENS}\n",
    ")\n",
    "print(f\"Added {num_added} special tokens: {SPECIAL_TOKENS}\")\n",
    "print(f\"New vocab size: {len(tokenizer)}\")\n",
    "\n",
    "\n",
    "# --- Checkpoint detection and loading ---\n",
    "from pathlib import Path\n",
    "\n",
    "start_epoch = 0\n",
    "training_losses = []\n",
    "checkpoint_path = Path(CHECKPOINT_DIR)\n",
    "\n",
    "\n",
    "def _load_model_from_dir(model_dir):\n",
    "    \"\"\"Load model, tokenizer, and optional training state from a local directory.\n",
    "    Returns (model, tokenizer, training_state_dict) or (None, None, None) if not found.\"\"\"\n",
    "    p = Path(model_dir)\n",
    "    if not (p / \"config.json\").exists():\n",
    "        return None, None, None\n",
    "    m = AutoModelForCausalLM.from_pretrained(\n",
    "        str(model_dir), dtype=torch.bfloat16\n",
    "    ).to(device)\n",
    "    # Load the saved tokenizer so its vocab (including special tokens) exactly\n",
    "    # matches the embedding matrix that was saved with the model.\n",
    "    tok = AutoTokenizer.from_pretrained(str(model_dir))\n",
    "    state = None\n",
    "    state_path = p / \"training_state.pt\"\n",
    "    if state_path.exists():\n",
    "        state = torch.load(state_path, map_location=device)\n",
    "    return m, tok, state\n",
    "\n",
    "\n",
    "loaded_model, loaded_tokenizer, training_state = None, None, None\n",
    "\n",
    "# # 1. Try local checkpoint\n",
    "# if checkpoint_path.exists():\n",
    "#     print(f\"Trying local checkpoint: {checkpoint_path}\")\n",
    "#     loaded_model, loaded_tokenizer, training_state = _load_model_from_dir(checkpoint_path)\n",
    "\n",
    "# # 2. Fall back to HF Hub\n",
    "# if loaded_model is None:\n",
    "#     try:\n",
    "#         from huggingface_hub import snapshot_download\n",
    "#         print(f\"Trying HF Hub checkpoint: {HF_CHECKPOINT_REPO}\")\n",
    "#         hf_dir = snapshot_download(HF_CHECKPOINT_REPO)\n",
    "#         loaded_model, loaded_tokenizer, training_state = _load_model_from_dir(hf_dir)\n",
    "#     except Exception as e:\n",
    "#         print(f\"No HF checkpoint available ({type(e).__name__}), starting fresh\")\n",
    "\n",
    "if loaded_model is not None:\n",
    "    base_model = loaded_model\n",
    "    tokenizer = loaded_tokenizer\n",
    "    if training_state is not None:\n",
    "        start_epoch = training_state.get(\"epoch\", 0)\n",
    "        training_losses = training_state.get(\"losses\", [])\n",
    "        print(f\"Resuming from epoch {start_epoch} ({len(training_losses)} epochs already logged)\")\n",
    "    else:\n",
    "        print(\"Checkpoint loaded; no training state found, starting from epoch 0\")\n",
    "else:\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "    print(\"No checkpoint found — starting fresh training\")\n",
    "\n",
    "# Wrap the model with the hierarchical CoT generate loop\n",
    "hier_model = CausalLMModelWithHierarchicalCot(base_model, tokenizer)\n",
    "print(f\"[THOUGHT] token id: {hier_model.thought_token_id}\")\n",
    "print(f\"[SOLUTION] token id: {hier_model.solution_token_id}\")\n",
    "print(f\"[RETURN] token id: {hier_model.return_token_id}\")\n",
    "\n",
    "\n",
    "# --- Training dataset ---\n",
    "class HierarchicalCoTDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_length=MAX_SEQ_LENGTH):\n",
    "        # Filter out examples with empty hierarchical_cot\n",
    "        self.data = [\n",
    "            ex for ex in hf_dataset if len(ex[\"hierarchical_cot\"].strip()) > 0\n",
    "        ]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        print(f\"  Training examples after filtering empty CoTs: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        prompt = PROMPT_TEMPLATE.format(problem=example[\"problem\"])\n",
    "        target = example[\"hierarchical_cot\"]\n",
    "\n",
    "        full_text = prompt + \"\\n\" + target + self.tokenizer.eos_token\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # Mask prompt tokens so loss is only on the hierarchical CoT\n",
    "        prompt_encoding = self.tokenizer(prompt + \"\\n\", return_tensors=\"pt\")\n",
    "        prompt_length = prompt_encoding[\"input_ids\"].shape[1]\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[:prompt_length] = -100\n",
    "        labels[attention_mask == 0] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaa42f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Memory tracing helpers ---\n",
    "def _mem(label=\"\"):\n",
    "    alloc    = torch.cuda.memory_allocated()  / 1e9\n",
    "    reserved = torch.cuda.memory_reserved()   / 1e9\n",
    "    peak     = torch.cuda.max_memory_allocated() / 1e9\n",
    "    total    = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"  [MEM] {label:<40} alloc={alloc:.2f}GB  reserved={reserved:.2f}GB  peak={peak:.2f}GB / {total:.1f}GB\")\n",
    "\n",
    "def _reset_peak():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "_mem(\"baseline (start of training cell)\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# STEP 1: Training setup\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# Gradient checkpointing recomputes activations during backward instead of\n",
    "# storing them — trades compute for memory.  Critical for long sequences.\n",
    "base_model.gradient_checkpointing_enable()\n",
    "\n",
    "train_ds = HierarchicalCoTDataset(train_data, tokenizer)\n",
    "\n",
    "lengths = [train_ds[i][\"attention_mask\"].sum().item() for i in range(len(train_ds))]\n",
    "print(f\"\\nActual token lengths:  min={min(lengths)}  max={max(lengths)}  mean={sum(lengths)/len(lengths):.0f}  padded_to={MAX_SEQ_LENGTH}\")\n",
    "print(f\"Padding overhead without dynamic collation: {MAX_SEQ_LENGTH / (sum(lengths)/len(lengths)):.1f}x\\n\")\n",
    "\n",
    "def dynamic_collate_fn(batch):\n",
    "    \"\"\"Pad each batch only to its longest real sequence (rounded up to nearest\n",
    "    128 tokens, the FlexAttention block size).  With batch_size=1 this is\n",
    "    simply the actual sequence length, eliminating the 4-5× padding overhead\n",
    "    from static MAX_SEQ_LENGTH padding.\"\"\"\n",
    "    max_real = max(b[\"attention_mask\"].sum().item() for b in batch)\n",
    "    # Round up to multiple of 128 so FlexAttention block grid is aligned\n",
    "    max_real = ((max_real + 127) // 128) * 128\n",
    "\n",
    "    return {\n",
    "        \"input_ids\":      torch.stack([b[\"input_ids\"][:max_real]      for b in batch]),\n",
    "        \"attention_mask\": torch.stack([b[\"attention_mask\"][:max_real] for b in batch]),\n",
    "        \"labels\":         torch.stack([b[\"labels\"][:max_real]         for b in batch]),\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=1, shuffle=True, collate_fn=dynamic_collate_fn\n",
    ")\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "if \"optimizer\" in globals():\n",
    "    del globals()[\"optimizer\"]\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    hier_model.parameters(), lr=LEARNING_RATE, weight_decay=0.01\n",
    ")\n",
    "_mem(\"after optimizer creation\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# STEP 2: Training loop\n",
    "# -----------------------------------------------------------------------\n",
    "hier_model.train()\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    epoch_losses = []\n",
    "    progress = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "\n",
    "    for step, batch in enumerate(progress):\n",
    "        trace = (epoch == start_epoch and step < 2)\n",
    "        if trace:\n",
    "            _reset_peak()\n",
    "\n",
    "        input_ids      = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels         = batch[\"labels\"].to(device)\n",
    "        real_len       = int(attention_mask.sum())\n",
    "\n",
    "        if trace: _mem(f\"[ep{epoch+1} s{step}] after .to(device)  real_len={real_len}\")\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        outputs = hier_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        if trace: _mem(f\"[ep{epoch+1} s{step}] after forward\")\n",
    "\n",
    "        loss = outputs.loss\n",
    "        del outputs\n",
    "\n",
    "        loss.backward()\n",
    "        if trace: _mem(f\"[ep{epoch+1} s{step}] after backward\")\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(hier_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "        progress.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        del loss\n",
    "\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    training_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} — Average Loss: {avg_loss:.4f}\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "\n",
    "checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "base_model.save_pretrained(checkpoint_path)\n",
    "tokenizer.save_pretrained(checkpoint_path)\n",
    "torch.save(\n",
    "    {\"epoch\": NUM_EPOCHS, \"losses\": training_losses},\n",
    "    checkpoint_path / \"training_state.pt\",\n",
    ")\n",
    "print(f\"Checkpoint saved to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "avdy066l756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push checkpoint to HuggingFace Hub\n",
    "# Requires: `huggingface-cli login`  OR  HF_TOKEN env variable set\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "print(f\"Pushing to https://huggingface.co/{HF_CHECKPOINT_REPO} ...\")\n",
    "base_model.push_to_hub(HF_CHECKPOINT_REPO, commit_message=\"Add hierarchical CoT fine-tuned model\")\n",
    "tokenizer.push_to_hub(HF_CHECKPOINT_REPO, commit_message=\"Add tokenizer with special tokens\")\n",
    "\n",
    "HfApi().upload_file(\n",
    "    path_or_fileobj=str(checkpoint_path / \"training_state.pt\"),\n",
    "    path_in_repo=\"training_state.pt\",\n",
    "    repo_id=HF_CHECKPOINT_REPO,\n",
    "    commit_message=\"Add training state\",\n",
    ")\n",
    "\n",
    "print(f\"Done! Model available at: https://huggingface.co/{HF_CHECKPOINT_REPO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f506cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the trained hierarchical model on the same test set\n",
    "hier_results = []\n",
    "hier_model.eval()\n",
    "\n",
    "for i, example in enumerate(tqdm(train_data, desc=\"Hierarchical inference\")):\n",
    "    problem = example[\"problem\"]\n",
    "    prompt = PROMPT_TEMPLATE.format(problem=problem)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    output_ids, tokens_generated, peak_context = hier_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "    )\n",
    "\n",
    "    # Decode with and without special tokens\n",
    "    output_with_special = tokenizer.decode(\n",
    "        output_ids[0][input_length:], skip_special_tokens=False\n",
    "    )\n",
    "    output_clean = tokenizer.decode(\n",
    "        output_ids[0][input_length:], skip_special_tokens=True\n",
    "    )\n",
    "    predicted_answer = extract_boxed_answer(output_clean)\n",
    "\n",
    "    hier_results.append(\n",
    "        {\n",
    "            \"problem\": problem,\n",
    "            \"output\": output_with_special,\n",
    "            \"output_clean\": output_clean,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"expected_answer\": example[\"expected_answer\"],\n",
    "            \"tokens_generated\": tokens_generated,\n",
    "            \"peak_context_length\": peak_context,\n",
    "            \"final_context_length\": output_ids.shape[1],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[Example {i}]\")\n",
    "    print(f\"  Problem:   {problem[:100]}...\")\n",
    "    print(f\"  Predicted: {predicted_answer}\")\n",
    "    print(f\"  Expected:  {example['expected_answer'][:80]}\")\n",
    "    print(f\"  Tokens generated: {tokens_generated}\")\n",
    "    print(f\"  Peak context:     {peak_context}\")\n",
    "    print(f\"  Final context:    {output_ids.shape[1]}\")\n",
    "\n",
    "# --- Side-by-side comparison ---\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"{'Metric':<30} {'Vanilla':>12} {'Hierarchical':>15}\")\n",
    "print(f\"{'=' * 60}\")\n",
    "v_gen = np.mean([r[\"tokens_generated\"] for r in vanilla_results])\n",
    "h_gen = np.mean([r[\"tokens_generated\"] for r in hier_results])\n",
    "v_ctx = np.mean([r[\"total_context_length\"] for r in vanilla_results])\n",
    "h_ctx = np.mean([r[\"final_context_length\"] for r in hier_results])\n",
    "h_peak = np.mean([r[\"peak_context_length\"] for r in hier_results])\n",
    "print(f\"{'Avg tokens generated':<30} {v_gen:>12.1f} {h_gen:>15.1f}\")\n",
    "print(f\"{'Avg context length':<30} {v_ctx:>12.1f} {h_ctx:>15.1f}\")\n",
    "print(f\"{'Avg peak context':<30} {'—':>12} {h_peak:>15.1f}\")\n",
    "print(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"Vanilla vs Hierarchical CoT Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "n = len(train_data)\n",
    "x = np.arange(n)\n",
    "width = 0.35\n",
    "\n",
    "# 1. Tokens generated per example\n",
    "ax = axes[0, 0]\n",
    "v_tokens = [r[\"tokens_generated\"] for r in vanilla_results]\n",
    "h_tokens = [r[\"tokens_generated\"] for r in hier_results]\n",
    "ax.bar(x - width / 2, v_tokens, width, label=\"Vanilla\", color=\"steelblue\", alpha=0.8)\n",
    "ax.bar(\n",
    "    x + width / 2, h_tokens, width, label=\"Hierarchical\", color=\"coral\", alpha=0.8\n",
    ")\n",
    "ax.set_xlabel(\"Test Example\")\n",
    "ax.set_ylabel(\"Tokens Generated\")\n",
    "ax.set_title(\"Tokens Generated per Example\")\n",
    "ax.set_xticks(x)\n",
    "ax.legend()\n",
    "\n",
    "# 2. Context window usage\n",
    "ax = axes[0, 1]\n",
    "v_context = [r[\"total_context_length\"] for r in vanilla_results]\n",
    "h_final = [r[\"final_context_length\"] for r in hier_results]\n",
    "h_peak = [r[\"peak_context_length\"] for r in hier_results]\n",
    "ax.bar(\n",
    "    x - width / 2,\n",
    "    v_context,\n",
    "    width,\n",
    "    label=\"Vanilla (total)\",\n",
    "    color=\"steelblue\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "ax.bar(\n",
    "    x + width / 2,\n",
    "    h_final,\n",
    "    width,\n",
    "    label=\"Hierarchical (final)\",\n",
    "    color=\"coral\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "ax.scatter(\n",
    "    x + width / 2,\n",
    "    h_peak,\n",
    "    color=\"red\",\n",
    "    zorder=5,\n",
    "    label=\"Hierarchical (peak)\",\n",
    "    marker=\"^\",\n",
    "    s=80,\n",
    ")\n",
    "ax.set_xlabel(\"Test Example\")\n",
    "ax.set_ylabel(\"Context Length (tokens)\")\n",
    "ax.set_title(\"Context Window Usage\")\n",
    "ax.set_xticks(x)\n",
    "ax.legend()\n",
    "\n",
    "# 3. Context reduction percentage\n",
    "ax = axes[1, 0]\n",
    "savings = [\n",
    "    (v - h) / v * 100 if v > 0 else 0 for v, h in zip(v_context, h_final)\n",
    "]\n",
    "colors = [\"green\" if s > 0 else \"red\" for s in savings]\n",
    "ax.bar(x, savings, color=colors, alpha=0.7)\n",
    "ax.axhline(y=0, color=\"black\", linewidth=0.5)\n",
    "if savings:\n",
    "    ax.axhline(\n",
    "        y=np.mean(savings),\n",
    "        color=\"blue\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Mean: {np.mean(savings):.1f}%\",\n",
    "    )\n",
    "ax.set_xlabel(\"Test Example\")\n",
    "ax.set_ylabel(\"Context Reduction (%)\")\n",
    "ax.set_title(\"Context Savings from Hierarchical Pruning\")\n",
    "ax.set_xticks(x)\n",
    "ax.legend()\n",
    "\n",
    "# 4. Training loss curve\n",
    "ax = axes[1, 1]\n",
    "ax.plot(\n",
    "    range(1, len(training_losses) + 1),\n",
    "    training_losses,\n",
    "    \"o-\",\n",
    "    color=\"purple\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Training Loss\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"verification_results.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Vanilla avg tokens generated:':<42} {np.mean(v_tokens):.1f}\")\n",
    "print(f\"{'Hierarchical avg tokens generated:':<42} {np.mean(h_tokens):.1f}\")\n",
    "print(f\"{'Vanilla avg context length:':<42} {np.mean(v_context):.1f}\")\n",
    "print(f\"{'Hierarchical avg final context:':<42} {np.mean(h_final):.1f}\")\n",
    "print(f\"{'Hierarchical avg peak context:':<42} {np.mean(h_peak):.1f}\")\n",
    "print(f\"{'Average context reduction:':<42} {np.mean(savings):.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
